{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "%pprint off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['backup0_1_550', 'backup1_550-1050', 'backup3_0628', 'backup2_0610']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/Users/songheekim/GoogleDrive/Primary/Projects/VerbVector/AMT/html/experiments/ratings2/results/'\n",
    "backups = next(os.walk(path))[1]\n",
    "backups = [b for b in backups if b.startswith('backup')]\n",
    "backups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_col(dataframe, col_to_move, reference_col, right=True):\n",
    "    col_list = dataframe.columns.values.tolist()\n",
    "    col_list2 = [x for x in col_list if x != col_to_move]\n",
    "    reference_idx = col_list2.index(reference_col)\n",
    "    if right==True:\n",
    "        col_list3 = []\n",
    "        for y in col_list2:\n",
    "            col_list3.append(y)\n",
    "            if y == reference_col:\n",
    "                col_list3.append(col_to_move)\n",
    "        return dataframe[col_list3]\n",
    "    else:\n",
    "        col_list3 = []\n",
    "        for y in col_list2:\n",
    "            col_list3.append(y)\n",
    "            if y == reference_col:\n",
    "                col_list3.insert(-2, col_to_move)\n",
    "        return  dataframe[col_list3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_res_catch_subj(backupfolder):\n",
    "    \n",
    "    path = '/Users/songheekim/GoogleDrive/Primary/Projects/VerbVector/AMT/html/experiments/ratings2/results/'\n",
    "#     stimr = pd.read_csv(path+backupfolder+'/'+'wro1_stimuli.txt', delimiter=\"\\t\")\n",
    "#     stim = stimr[stimr['id'].notna()][[\"id\", \"stim\"]].reset_index(drop=True).astype({\"id\": int})\n",
    "#     stim['stim'] = stim['stim'].str[:-1]\n",
    "\n",
    "    catchr = pd.read_csv(path+backupfolder+'/'+'wro1_catchresults.txt',delimiter=\"\\t\")\n",
    "    catch = catchr.rename(columns={\"response\": \"catch_response\", \"subject_id\":\"catch_subject_id\"})\n",
    "\n",
    "    resr = pd.read_csv(path+backupfolder+'/'+'wro1_results.txt', delimiter=\"\\t\")\n",
    "    res = resr.drop(columns=['stimtype_id','sense'])\n",
    "\n",
    "    subj_raw = pd.read_csv(path+backupfolder+'/'+'wro1_subjects.txt',delimiter=\"\\t\")\n",
    "    subj = subj_raw.rename(columns={\"id\": \"subject_id\"})\n",
    "\n",
    "    return (res, catch, subj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backup selected: backup0_1_550\n",
      "['.DS_Store', 'wro1_stimuli.txt', 'wro1_subjects.txt', 'wro1_catchresults.txt', 'wro1_stimtypes.txt', 'wro1_results.txt', 'wro1_questions.txt']\n",
      "FROM 2021-05-25 16:26:41 TO 2021-05-28 01:18:52\n",
      "# of questions: 72\n",
      "# of catch HITs: 554\n",
      "# of HITs:      554\n",
      "# of subjects:  527\n",
      "=============================================\n",
      "backup selected: backup3_0628\n",
      "['wro1_stimuli copy.txt', '.DS_Store', 'wro1_stimuli.txt', 'wro1_subjects.txt', 'wro1_catchresults.txt', 'wro1_results.txt', 'wro1_questions.txt']\n",
      "FROM 2021-05-28 16:52:09 TO 2021-06-23 20:57:21\n",
      "# of questions: 72\n",
      "# of catch HITs: 2262\n",
      "# of HITs:      2262\n",
      "# of subjects:  898\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "#### basic check on each dataset ##### \n",
    "b1 = backups[0]\n",
    "b2 = backups[2]\n",
    "\n",
    "for b in [b1, b2]:\n",
    "    print('backup selected:', b)\n",
    "    print(os.listdir(path+'/'+b))\n",
    "\n",
    "    stimr = pd.read_csv(path+b+'/'+'wro1_stimuli.txt', delimiter=\"\\t\")\n",
    "    stim = stimr[stimr['id'].notna()][[\"id\", \"stim\"]].reset_index(drop=True).astype({\"id\": int})\n",
    "    stim['stim'] = stim['stim'].str[:-1]\n",
    "    \n",
    "    ques = pd.read_csv(path+b+'/'+'wro1_questions.txt', delimiter=\"\\t\")\n",
    "\n",
    "    catchr = pd.read_csv(path+b+'/'+'wro1_catchresults.txt',delimiter=\"\\t\")\n",
    "    catch = catchr.rename(columns={\"response\": \"catch_response\", \"subject_id\":\"catch_subject_id\"})\n",
    "    #print (\"catch trial:\", catch.shape[0])\n",
    "    \n",
    "    resr = pd.read_csv(path+b+'/'+'wro1_results.txt', delimiter=\"\\t\")\n",
    "    res = resr.drop(columns=['stimtype_id','sense'])\n",
    "    #print (\"main trial:\", res.shape[0])\n",
    "    starttime = res.loc[res['id']==1, \"start\"].values[0]\n",
    "    endtime = res.loc[res['id']==res.shape[0], \"end\"].values[0]\n",
    "    print('FROM', starttime, 'TO', endtime)\n",
    "    \n",
    "    subj_raw = pd.read_csv(path+b+'/'+'wro1_subjects.txt',delimiter=\"\\t\")\n",
    "    subj = subj_raw.rename(columns={\"id\": \"subject_id\"})\n",
    "    #print('subject N:', subj.shape[0])\n",
    "    \n",
    "    print('# of questions:', ques.shape[0])\n",
    "    print('# of catch HITs:', catch.shape[0])\n",
    "    print('# of HITs:     ', res.shape[0])\n",
    "    print('# of subjects: ', subj.shape[0])\n",
    "    \n",
    "    check1 = catch.shape[0] == res.shape[0]\n",
    "    check2 = res.shape[0] >= subj.shape[0] \n",
    "    check3 = res['subject_id'].equals(other=catch['catch_subject_id'])\n",
    "\n",
    "    if check1==0:\n",
    "        print ('# of catch trials != # of trials')\n",
    "    elif check2==0:\n",
    "        print ('# of subjects are more than # of HITs')\n",
    "    elif check3==0:\n",
    "        print('subject_id are not the same in trials and catchtrials')\n",
    "       \n",
    "    print ('=============================================') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######catch trial info\n",
    "catch1 = 'being an animal living on the surface of sun'\n",
    "catch2 = 'being something about which you are supposed to answer questions in this assignment'\n",
    "catch3 = 'being an object that is smaller than a shoe box and larger than a mountain'\n",
    "catch4 = 'being something you have thought about within the last couple minutes'\n",
    "catch5 = 'being used to check whether you are carefully reading the questions'\n",
    "catch_answers = ['0/7','6','0','6','6']\n",
    "\n",
    "def count_correct_answers(list1):\n",
    "    '''list1 = correct answers, list2=actual responses'''\n",
    "    catch_answers = ['0/7','6','0','6','6']\n",
    "    if len(list1) != len(catch_answers):\n",
    "        print ('responses have different lengths!')\n",
    "        return\n",
    "    correct_answer = 0\n",
    "    if catch_answers[0] in list1[0].split('/'):\n",
    "        correct_answer += 1\n",
    "    for i in range(1,len(list1)):\n",
    "        iscorrect = list1[i]==catch_answers[i]\n",
    "        correct_answer += iscorrect        \n",
    "    return correct_answer\n",
    "\n",
    "count_correct_answers(['5', '6', '0', '6', '6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_results(res, catch, subj): \n",
    "    res_all = pd.merge(left=res, right=subj, on='subject_id')\n",
    "    res_all = res_all.sort_values(by=['id'])\n",
    "    print('result df:', res.shape)\n",
    "    print('subject df:', subj.shape)\n",
    "    print('df returned:', res_all.shape)\n",
    "    \n",
    "    catch = catch.drop(columns='id') \n",
    "    res_all = pd.merge(left=res_all, right=catch, left_on=\"id\", right_on=\"results_id\")\n",
    "    \n",
    "    #print(res_all.shape)\n",
    "    #print(catch.shape)\n",
    "    print ('=========================================================')\n",
    "    \n",
    "    return res_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result df: (554, 9)\n",
      "subject df: (527, 6)\n",
      "df returned: (554, 14)\n",
      "=========================================================\n",
      "result df: (2262, 9)\n",
      "subject df: (898, 6)\n",
      "df returned: (2262, 14)\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "## merge each data \n",
    "[res1, catch1, subj1] = return_res_catch_subj(b1)\n",
    "[res2, catch2, subj2] = return_res_catch_subj(b2)\n",
    "res_all1 = merge_results(res1, catch1, subj1)\n",
    "res_all2 = merge_results(res2, catch2, subj2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(554, 17)\n",
      "(2262, 17)\n",
      "(2816, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>stim_id</th>\n",
       "      <th>stim</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>response</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>rt</th>\n",
       "      <th>buttonpress</th>\n",
       "      <th>turkcode</th>\n",
       "      <th>subject_num</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>occupation</th>\n",
       "      <th>results_id</th>\n",
       "      <th>catch_subject_id</th>\n",
       "      <th>catch_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>159</td>\n",
       "      <td>laugh</td>\n",
       "      <td>1</td>\n",
       "      <td>1_0_0_3_0_0_1_5_6_2_3_3_6_2_0_0_0_0_0_0_0_0_2_...</td>\n",
       "      <td>2021-05-25 16:26:41</td>\n",
       "      <td>2021-05-25 16:35:15</td>\n",
       "      <td>10451_2940_2319_3999_2779_3110_6525_7596_3985_...</td>\n",
       "      <td>1_1_1_1_1_1_1_1_1_1_2_3_1_1_1_1_1_1_1_1_1_1_1_...</td>\n",
       "      <td>8545510</td>\n",
       "      <td>A3I9XLIHPPWPN1</td>\n",
       "      <td>f</td>\n",
       "      <td>57</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Dataanalystadministrationassistant</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0_6_0_6_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>295</td>\n",
       "      <td>see</td>\n",
       "      <td>2</td>\n",
       "      <td>6_3_0_1_1_2_0_4_0_1_0_3_3_0_0_0_0_0_0_0_0_0_0_...</td>\n",
       "      <td>2021-05-25 16:29:38</td>\n",
       "      <td>2021-05-25 16:36:24</td>\n",
       "      <td>4024_9148_2726_5130_5199_3487_2790_4329_4503_2...</td>\n",
       "      <td>1_4_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_...</td>\n",
       "      <td>2727964</td>\n",
       "      <td>A2CWJRAEFZ44HU</td>\n",
       "      <td>m</td>\n",
       "      <td>36</td>\n",
       "      <td>14.0</td>\n",
       "      <td>nursingassistantihelpoutpatientsandstaffinahos...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0_6_0_6_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>214</td>\n",
       "      <td>flatten</td>\n",
       "      <td>3</td>\n",
       "      <td>3_7_7_7_5_3_1_1_3_2_0_5_7_1_4_0_0_0_0_0_1_0_7_...</td>\n",
       "      <td>2021-05-25 16:26:58</td>\n",
       "      <td>2021-05-25 16:36:47</td>\n",
       "      <td>4432_2912_2592_3640_7976_6904_5368_6088_13600_...</td>\n",
       "      <td>1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_2_1_...</td>\n",
       "      <td>3205581</td>\n",
       "      <td>A7ERZELTAMWL5</td>\n",
       "      <td>m</td>\n",
       "      <td>70</td>\n",
       "      <td>18.0</td>\n",
       "      <td>PatrolofficerIridearoundaresidentialcomplextoc...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0_6_0_6_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>195</td>\n",
       "      <td>increase</td>\n",
       "      <td>4</td>\n",
       "      <td>4_3_6_4_3_2_3_3_6_5_5_5_6_6_5_5_6_3_6_6_4_6_6_...</td>\n",
       "      <td>2021-05-25 16:30:25</td>\n",
       "      <td>2021-05-25 16:37:35</td>\n",
       "      <td>2941_3771_4392_2726_3770_3690_2473_1306_1137_1...</td>\n",
       "      <td>1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_2_1_1_...</td>\n",
       "      <td>6434648</td>\n",
       "      <td>A1MKYMYY34DZIO</td>\n",
       "      <td>m</td>\n",
       "      <td>30</td>\n",
       "      <td>16.0</td>\n",
       "      <td>student</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4_6_5_6_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>calm</td>\n",
       "      <td>5</td>\n",
       "      <td>4_0_0_0_3_0_0_1_4_0_5_0_4_2_2_2_0_1_0_0_0_0_3_...</td>\n",
       "      <td>2021-05-25 16:28:07</td>\n",
       "      <td>2021-05-25 16:38:09</td>\n",
       "      <td>4142_2349_7257_8933_4802_3372_3349_8304_4593_1...</td>\n",
       "      <td>1_1_1_1_1_1_1_1_1_1_2_1_2_1_2_2_1_1_1_1_1_1_1_...</td>\n",
       "      <td>1247441</td>\n",
       "      <td>A2LF84L3K71GR2</td>\n",
       "      <td>f</td>\n",
       "      <td>38</td>\n",
       "      <td>12.0</td>\n",
       "      <td>RestaurantSupervisor</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0_6_0_6_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090</th>\n",
       "      <td>2812</td>\n",
       "      <td>241</td>\n",
       "      <td>specify</td>\n",
       "      <td>67</td>\n",
       "      <td>5_6_3_0_6_0_3_1_0_3_2_5_1_3_5_3_1_5_4_4_2_6_6_...</td>\n",
       "      <td>2021-06-23 16:53:51</td>\n",
       "      <td>2021-06-23 17:02:21</td>\n",
       "      <td>3708_4464_2788_2388_2708_4596_8992_2489_6216_2...</td>\n",
       "      <td>1_1_1_1_1_1_1_1_2_1_1_1_1_1_1_1_1_1_2_1_1_1_1_...</td>\n",
       "      <td>2268511</td>\n",
       "      <td>A2WPHVMLLEV5ZB</td>\n",
       "      <td>m</td>\n",
       "      <td>26</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Mturker</td>\n",
       "      <td>2258</td>\n",
       "      <td>67</td>\n",
       "      <td>0_6_0_6_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>2813</td>\n",
       "      <td>80</td>\n",
       "      <td>wilt</td>\n",
       "      <td>280</td>\n",
       "      <td>0_0_0_5_4_0_0_3_3_0_3_3_0_0_0_0_0_0_0_0_0_1_0_...</td>\n",
       "      <td>2021-06-23 16:49:42</td>\n",
       "      <td>2021-06-23 17:02:27</td>\n",
       "      <td>15064_2207_9752_2816_6272_5664_4656_78280_1321...</td>\n",
       "      <td>1_1_1_1_1_1_1_2_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_...</td>\n",
       "      <td>9427636</td>\n",
       "      <td>A1P57CKX00HC2I</td>\n",
       "      <td>f</td>\n",
       "      <td>63</td>\n",
       "      <td>26.0</td>\n",
       "      <td>selfemployed</td>\n",
       "      <td>2259</td>\n",
       "      <td>280</td>\n",
       "      <td>0_6_0_6_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>2814</td>\n",
       "      <td>32</td>\n",
       "      <td>squeal</td>\n",
       "      <td>552</td>\n",
       "      <td>0_0_0_0_0_0_3_3_1_0_1_0_1_0_0_3_0_0_0_0_0_1_6_...</td>\n",
       "      <td>2021-06-23 17:01:15</td>\n",
       "      <td>2021-06-23 17:06:45</td>\n",
       "      <td>3222_1628_2109_2202_2187_2383_5384_2179_5352_4...</td>\n",
       "      <td>1_1_1_1_1_1_1_1_2_1_1_1_1_1_1_1_1_1_1_1_1_2_1_...</td>\n",
       "      <td>8197779</td>\n",
       "      <td>A2OVX9UW5WANQE</td>\n",
       "      <td>f</td>\n",
       "      <td>47</td>\n",
       "      <td>14.0</td>\n",
       "      <td>secretary</td>\n",
       "      <td>2260</td>\n",
       "      <td>552</td>\n",
       "      <td>0_6_0_6_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1841</th>\n",
       "      <td>2815</td>\n",
       "      <td>221</td>\n",
       "      <td>darken</td>\n",
       "      <td>898</td>\n",
       "      <td>5_0_6_6_6_0_0_0_3_0_3_0_2_5_0_0_1_0_0_0_2_3_0_...</td>\n",
       "      <td>2021-06-23 17:06:28</td>\n",
       "      <td>2021-06-23 17:17:33</td>\n",
       "      <td>5103_16967_6294_4914_6429_4531_14854_6642_6963...</td>\n",
       "      <td>1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_...</td>\n",
       "      <td>3528996</td>\n",
       "      <td>A12MV66U9VGLQ1</td>\n",
       "      <td>f</td>\n",
       "      <td>81</td>\n",
       "      <td>19.0</td>\n",
       "      <td>retiredcomputerprogrammer</td>\n",
       "      <td>2261</td>\n",
       "      <td>898</td>\n",
       "      <td>7_6_7_6_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2780</th>\n",
       "      <td>2816</td>\n",
       "      <td>249</td>\n",
       "      <td>announce</td>\n",
       "      <td>548</td>\n",
       "      <td>0_7_0_0_0_0_7_0_2_0_7_0_3_0_0_0_0_0_0_0_7_0_6_...</td>\n",
       "      <td>2021-06-23 20:48:05</td>\n",
       "      <td>2021-06-23 20:57:21</td>\n",
       "      <td>2107_1433_1765_7331_1426_1410_2358_29085_3190_...</td>\n",
       "      <td>1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_...</td>\n",
       "      <td>4240846</td>\n",
       "      <td>A34CPKFZXBX1PO</td>\n",
       "      <td>f</td>\n",
       "      <td>22</td>\n",
       "      <td>16.0</td>\n",
       "      <td>receptionist</td>\n",
       "      <td>2262</td>\n",
       "      <td>548</td>\n",
       "      <td>0_6_0_6_6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2816 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  stim_id      stim  subject_id  \\\n",
       "0        1      159     laugh           1   \n",
       "10       2      295       see           2   \n",
       "18       3      214   flatten           3   \n",
       "26       4      195  increase           4   \n",
       "35       5      262      calm           5   \n",
       "...    ...      ...       ...         ...   \n",
       "2090  2812      241   specify          67   \n",
       "207   2813       80      wilt         280   \n",
       "555   2814       32    squeal         552   \n",
       "1841  2815      221    darken         898   \n",
       "2780  2816      249  announce         548   \n",
       "\n",
       "                                               response                start  \\\n",
       "0     1_0_0_3_0_0_1_5_6_2_3_3_6_2_0_0_0_0_0_0_0_0_2_...  2021-05-25 16:26:41   \n",
       "10    6_3_0_1_1_2_0_4_0_1_0_3_3_0_0_0_0_0_0_0_0_0_0_...  2021-05-25 16:29:38   \n",
       "18    3_7_7_7_5_3_1_1_3_2_0_5_7_1_4_0_0_0_0_0_1_0_7_...  2021-05-25 16:26:58   \n",
       "26    4_3_6_4_3_2_3_3_6_5_5_5_6_6_5_5_6_3_6_6_4_6_6_...  2021-05-25 16:30:25   \n",
       "35    4_0_0_0_3_0_0_1_4_0_5_0_4_2_2_2_0_1_0_0_0_0_3_...  2021-05-25 16:28:07   \n",
       "...                                                 ...                  ...   \n",
       "2090  5_6_3_0_6_0_3_1_0_3_2_5_1_3_5_3_1_5_4_4_2_6_6_...  2021-06-23 16:53:51   \n",
       "207   0_0_0_5_4_0_0_3_3_0_3_3_0_0_0_0_0_0_0_0_0_1_0_...  2021-06-23 16:49:42   \n",
       "555   0_0_0_0_0_0_3_3_1_0_1_0_1_0_0_3_0_0_0_0_0_1_6_...  2021-06-23 17:01:15   \n",
       "1841  5_0_6_6_6_0_0_0_3_0_3_0_2_5_0_0_1_0_0_0_2_3_0_...  2021-06-23 17:06:28   \n",
       "2780  0_7_0_0_0_0_7_0_2_0_7_0_3_0_0_0_0_0_0_0_7_0_6_...  2021-06-23 20:48:05   \n",
       "\n",
       "                      end                                                 rt  \\\n",
       "0     2021-05-25 16:35:15  10451_2940_2319_3999_2779_3110_6525_7596_3985_...   \n",
       "10    2021-05-25 16:36:24  4024_9148_2726_5130_5199_3487_2790_4329_4503_2...   \n",
       "18    2021-05-25 16:36:47  4432_2912_2592_3640_7976_6904_5368_6088_13600_...   \n",
       "26    2021-05-25 16:37:35  2941_3771_4392_2726_3770_3690_2473_1306_1137_1...   \n",
       "35    2021-05-25 16:38:09  4142_2349_7257_8933_4802_3372_3349_8304_4593_1...   \n",
       "...                   ...                                                ...   \n",
       "2090  2021-06-23 17:02:21  3708_4464_2788_2388_2708_4596_8992_2489_6216_2...   \n",
       "207   2021-06-23 17:02:27  15064_2207_9752_2816_6272_5664_4656_78280_1321...   \n",
       "555   2021-06-23 17:06:45  3222_1628_2109_2202_2187_2383_5384_2179_5352_4...   \n",
       "1841  2021-06-23 17:17:33  5103_16967_6294_4914_6429_4531_14854_6642_6963...   \n",
       "2780  2021-06-23 20:57:21  2107_1433_1765_7331_1426_1410_2358_29085_3190_...   \n",
       "\n",
       "                                            buttonpress  turkcode  \\\n",
       "0     1_1_1_1_1_1_1_1_1_1_2_3_1_1_1_1_1_1_1_1_1_1_1_...   8545510   \n",
       "10    1_4_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_...   2727964   \n",
       "18    1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_2_1_...   3205581   \n",
       "26    1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_2_1_1_...   6434648   \n",
       "35    1_1_1_1_1_1_1_1_1_1_2_1_2_1_2_2_1_1_1_1_1_1_1_...   1247441   \n",
       "...                                                 ...       ...   \n",
       "2090  1_1_1_1_1_1_1_1_2_1_1_1_1_1_1_1_1_1_2_1_1_1_1_...   2268511   \n",
       "207   1_1_1_1_1_1_1_2_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_...   9427636   \n",
       "555   1_1_1_1_1_1_1_1_2_1_1_1_1_1_1_1_1_1_1_1_1_2_1_...   8197779   \n",
       "1841  1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_...   3528996   \n",
       "2780  1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_...   4240846   \n",
       "\n",
       "         subject_num gender  age  education  \\\n",
       "0     A3I9XLIHPPWPN1      f   57       18.0   \n",
       "10    A2CWJRAEFZ44HU      m   36       14.0   \n",
       "18     A7ERZELTAMWL5      m   70       18.0   \n",
       "26    A1MKYMYY34DZIO      m   30       16.0   \n",
       "35    A2LF84L3K71GR2      f   38       12.0   \n",
       "...              ...    ...  ...        ...   \n",
       "2090  A2WPHVMLLEV5ZB      m   26       12.0   \n",
       "207   A1P57CKX00HC2I      f   63       26.0   \n",
       "555   A2OVX9UW5WANQE      f   47       14.0   \n",
       "1841  A12MV66U9VGLQ1      f   81       19.0   \n",
       "2780  A34CPKFZXBX1PO      f   22       16.0   \n",
       "\n",
       "                                             occupation  results_id  \\\n",
       "0                    Dataanalystadministrationassistant           1   \n",
       "10    nursingassistantihelpoutpatientsandstaffinahos...           2   \n",
       "18    PatrolofficerIridearoundaresidentialcomplextoc...           3   \n",
       "26                                              student           4   \n",
       "35                                 RestaurantSupervisor           5   \n",
       "...                                                 ...         ...   \n",
       "2090                                            Mturker        2258   \n",
       "207                                        selfemployed        2259   \n",
       "555                                           secretary        2260   \n",
       "1841                          retiredcomputerprogrammer        2261   \n",
       "2780                                       receptionist        2262   \n",
       "\n",
       "      catch_subject_id catch_response  \n",
       "0                    1      0_6_0_6_6  \n",
       "10                   2      0_6_0_6_6  \n",
       "18                   3      0_6_0_6_6  \n",
       "26                   4      4_6_5_6_6  \n",
       "35                   5      0_6_0_6_6  \n",
       "...                ...            ...  \n",
       "2090                67      0_6_0_6_6  \n",
       "207                280      0_6_0_6_6  \n",
       "555                552      0_6_0_6_6  \n",
       "1841               898      7_6_7_6_6  \n",
       "2780               548      0_6_0_6_6  \n",
       "\n",
       "[2816 rows x 18 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_combined = pd.concat([res_all1, res_all2], ignore_index=True)\n",
    "res_combined['id'] = list(range(1, res_combined.shape[0]+1))\n",
    "\n",
    "print(res_all1.shape)\n",
    "print(res_all2.shape)\n",
    "print(res_combined.shape)\n",
    "#res_combined\n",
    "\n",
    "###check whether there is a duplicate row\n",
    "# duplicates = res_combined.duplicated()\n",
    "# duplicates = duplicates.index[duplicates]\n",
    "# len(duplicates)\n",
    "\n",
    "\n",
    "#### add lemma column\n",
    "stimr = pd.read_csv(path+b1+'/'+'wro1_stimuli.txt', delimiter=\"\\t\")\n",
    "stim = stimr[stimr['id'].notna()][[\"id\", \"stim\"]].reset_index(drop=True).astype({\"id\": int})\n",
    "stim['stim'] = stim['stim'].str[:-1]\n",
    "stim.rename(columns={'id':'stim_id'}, inplace=True)\n",
    "#stim\n",
    "\n",
    "res_combined = pd.merge(left=res_combined, right=stim, on=\"stim_id\").sort_values(by=\"id\")\n",
    "res_combined = rearrange_col(res_combined, 'stim', 'stim_id')\n",
    "res_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########test\n",
    "# df = pd.DataFrame({'id':[1,2,3,4,5,6,7,8], 'stim_id': [1,2,3,1,2,3,7,6]})\n",
    "# st = pd.DataFrame({'stim_id': [1,2,3,4,5,6,7,8,9], 'stim': ['a','b','c','d','e','f','g','h','i']})\n",
    "# df\n",
    "\n",
    "# st\n",
    "\n",
    "# dfst = pd.merge(left=df, right=st, on=\"stim_id\").sort_values(by=\"id\")\n",
    "# dfst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of bad subject: 358 out of 2816\n"
     ]
    }
   ],
   "source": [
    "##### mark bad subjects\n",
    "res_combined['catch_response_split'] = res_combined['catch_response'].apply(lambda x:x.split('_'))\n",
    "catch_series = res_combined['catch_response_split'].tolist()\n",
    "correct_answer = [count_correct_answers(l) for l in catch_series]\n",
    "res_combined['correctN'] = correct_answer\n",
    "\n",
    "res_combined.loc[res_combined['correctN']<4, 'badsubject'] = 'Y'\n",
    "res_combined.loc[res_combined['correctN']>=4, 'badsubject'] = 'N'\n",
    "res_bad = res_combined.loc[res_combined['badsubject']=='Y']\n",
    "print('# of bad subject:', res_bad.shape[0], 'out of', res_combined.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_combined.drop(columns=['catch_response_split'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## export to csv\n",
    "res_combined.to_csv(path+'results_clean_0628.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######to export bad subjects list\n",
    "# df_out = res_combined.loc[res_combined['badsubject']=='Y', ]\n",
    "# df_out.to_csv(path+'badsubjects.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
